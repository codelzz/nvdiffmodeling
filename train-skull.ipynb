{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fadbe54-d3f8-4355-9f21-3690cfbd6c3c",
   "metadata": {},
   "source": [
    "# Nvdiffmodeling 算法测试\n",
    "\n",
    "[GitHub](https://github.com/NVlabs/nvdiffmodeling) | [Paper](https://research.nvidia.com/publication/2021-04_appearance-driven-automatic-3d-model-simplification) | [Video](https://d1qx31qr3h6wln.cloudfront.net/publications/autolod_video.mp4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3f2399-0dd2-46b6-b505-4316a6c9b677",
   "metadata": {},
   "source": [
    "问题：    \n",
    "1. 为什么Radius参数会用于camera eye位置初始化？\n",
    "2. 为什么加载json配置文件覆盖camera eye设置后不需要变更对应的radius？\n",
    "3. FLAGS.layers 代表什么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "128d7fc4-9bd5-4b22-a696-35fae0029eb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using C:\\Users\\zhong\\AppData\\Local\\torch_extensions\\torch_extensions\\Cache\\py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file C:\\Users\\zhong\\AppData\\Local\\torch_extensions\\torch_extensions\\Cache\\py310_cu121\\renderutils_plugin\\build.ninja...\n",
      "Building extension module renderutils_plugin...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module renderutils_plugin...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import nvdiffrast.torch as dr\n",
    "\n",
    "import src.renderutils as ru\n",
    "from src import obj\n",
    "from src import util\n",
    "from src import mesh\n",
    "from src import texture\n",
    "from src import render\n",
    "from src import regularizer\n",
    "from src.mesh import Mesh\n",
    "from src import helper\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa04335-8014-4dc7-bc59-6b02e093d009",
   "metadata": {},
   "source": [
    "## 全局参数加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a254dc66-4d95-4756-89ac-f06680d4782a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----配置信息----\n",
      "迭代次数(iter):10000\n",
      "批大小(batch):8\n",
      "单像素采样数(spp):1\n",
      "层数(layers):1\n",
      "训练分辨率(train_res):512\n",
      "随机训练分辨率(random_train_res):False\n",
      "显示分辨率(display_res):512\n",
      "材质分辨率(texture_res):[2048, 2048]\n",
      "显示间隔(display_interval):0\n",
      "保存间隔(save_interval):1000\n",
      "学习率(learning_rate):0.01\n",
      "光功率(light_power):5.0\n",
      "最小粗糙度(min_roughness):0.08\n",
      "细分(subdivision):0\n",
      "自定义mip(custom_mip):False\n",
      "随机纹理(random_textures):False\n",
      "拉普拉斯因子(laplacian_factor):None\n",
      "相对拉普拉斯(relative_laplacian):False\n",
      "背景(background):checker\n",
      "损失函数(loss):logl1\n",
      "输出文件目录(out_dir):out/skull\n",
      "配置文件(config):configs/skull.json\n",
      "参考网格(ref_mesh):data/skull/skull_uv.obj\n",
      "基础网格(base_mesh):data/skull/skull_p400_LOD1.obj\n",
      "相机位置(camera_eye):[2.5, 0.0, 2.5]\n",
      "相机朝向(camera_up):[0.0, 1.0, 0.0]\n",
      "弧度(radius):3.5\n",
      "投影矩阵(proj_mtx):[[ 2.5       0.        0.        0.      ]\n",
      " [ 0.       -2.5       0.        0.      ]\n",
      " [ 0.        0.       -1.002002 -2.002002]\n",
      " [ 0.        0.       -1.        0.      ]]\n",
      "跳过训练(skip_train):['kd', 'ks']\n",
      "位移(displacement):0.15\n",
      "材质重写(mtl_override):None\n"
     ]
    }
   ],
   "source": [
    "CONFIG_FILE = \"configs/skull.json\"\n",
    "FLAGS = helper.init_flags(args=['--config', CONFIG_FILE])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d716c7-18c5-429c-9dec-a0a96fa09b29",
   "metadata": {},
   "source": [
    "## 数据加载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e6bbda-8fa1-49d0-811b-24e3e0cc799e",
   "metadata": {},
   "source": [
    "### 创建输出文件目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3300b999-55f7-411e-b410-a3467b8f04bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(FLAGS.out_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(FLAGS.out_dir, \"mesh\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714a651e-5109-4213-98c5-b3c9b6bcd45c",
   "metadata": {},
   "source": [
    "### 网格加载（Mesh Loading）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08739071-00cb-405f-9728-a1e55e1f861b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参考网格包含 735034 三角面和 2205100 顶点，平均边长为：0.0174\n",
      "基础网格包含 9058 三角面和 4434 顶点，平均边长为：0.1376\n",
      "预期减面率为: 98.77%\n",
      "预期减点率为: 99.80%\n"
     ]
    }
   ],
   "source": [
    "# 加载参考网格\n",
    "ref_mesh = helper.load_mesh(FLAGS.ref_mesh, FLAGS.mtl_override)\n",
    "print(\"参考网格包含 %d 三角面和 %d 顶点，平均边长为：%.4f\" % (ref_mesh.t_pos_idx.shape[0], ref_mesh.v_pos.shape[0], regularizer.avg_edge_length(ref_mesh)))\n",
    "\n",
    "# 加载基础网格\n",
    "base_mesh = helper.load_mesh(FLAGS.base_mesh)\n",
    "print(\"基础网格包含 %d 三角面和 %d 顶点，平均边长为：%.4f\" % (base_mesh.t_pos_idx.shape[0], base_mesh.v_pos.shape[0], regularizer.avg_edge_length(base_mesh)))\n",
    "\n",
    "def check_resolution(ref_mesh, texture_res):\n",
    "    # 检测训练纹理的分辨率是否合规\n",
    "    # Check if the training texture resolution is acceptable\n",
    "    ref_texture_res = np.maximum(ref_mesh.material['kd'].getRes(), ref_mesh.material['ks'].getRes())\n",
    "    if 'normal' in ref_mesh.material:\n",
    "        ref_texture_res = np.maximum(ref_texture_res, ref_mesh.material['normal'].getRes())\n",
    "    if texture_res[0] < ref_texture_res[0] or texture_res[1] < ref_texture_res[1]:\n",
    "        print(\"---> 警告: 所选纹理分辨率小于参考网格纹理分辨率 [%d, %d] < [%d, %d]\" % (texture_res[0], texture_res[1], ref_texture_res[0], ref_texture_res[1]))\n",
    "\n",
    "check_resolution(ref_mesh, FLAGS.texture_res)\n",
    "\n",
    "# 计算预期优化效果\n",
    "t_reduction_rate = (1 - base_mesh.t_pos_idx.shape[0] / ref_mesh.t_pos_idx.shape[0]) * 100\n",
    "v_reduction_rate = (1 - base_mesh.v_pos.shape[0] / ref_mesh.v_pos.shape[0]) * 100\n",
    "print(\"预期减面率为: %.2f%%\" % t_reduction_rate)\n",
    "print(\"预期减点率为: %.2f%%\" % v_reduction_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "40da4675-718d-4196-8bea-9e137402740d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3609, 0.2383],\n",
       "        [0.3599, 0.2383],\n",
       "        [0.3604, 0.2388],\n",
       "        ...,\n",
       "        [0.1388, 0.8414],\n",
       "        [0.1393, 0.8409],\n",
       "        [0.1388, 0.8404]], device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_mesh.v_tex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b79581f-cec6-405f-a680-326bc787f347",
   "metadata": {},
   "source": [
    "### 网格归一化（Mesh Normalization）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "937f690c-a897-43a4-afbd-16d2f0901918",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalized_base_mesh = mesh.unit_size(base_mesh)\n",
    "normalized_ref_mesh = mesh.unit_size(ref_mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004b6f05-7e10-4166-b6a8-d7f569c1085a",
   "metadata": {},
   "source": [
    "## 初始化训练网格的权重和变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4290bf81-b5aa-4610-9e93-e37d4474254e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0212, -0.2247,  0.9920],\n",
       "        [-0.0818,  0.1746, -0.9296],\n",
       "        [-0.2772, -0.1834, -0.8503],\n",
       "        ...,\n",
       "        [ 0.0214, -0.2145,  0.9918],\n",
       "        [ 0.0036, -0.2239,  0.9999],\n",
       "        [-0.0346, -0.2175,  1.0000]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_list = []\n",
    "# 归一化基础网格顶点\n",
    "v_pos_opt = normalized_base_mesh.v_pos.clone().detach().requires_grad_(True)\n",
    "v_pos_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888164bb-98d6-4261-9f75-f7ffc03c5ced",
   "metadata": {},
   "source": [
    "#### 构建可训练的法线贴图，初始化为(0,0,1)，并确保法向量总处于正半球。\n",
    "Trainable normal map, initialize to (0,0,1) & make sure normals are always in positive hemisphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2622b174-a9d1-4949-af07-e4f4fb23db88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if FLAGS.random_textures:\n",
    "    normal_map_opt = texture.create_trainable(np.array([0, 0, 1]), FLAGS.texture_res, not FLAGS.custom_mip)\n",
    "else:\n",
    "    if 'normal' not in ref_mesh.material:\n",
    "        normal_map_opt = texture.create_trainable(np.array([0, 0, 1]), FLAGS.texture_res, not FLAGS.custom_mip)\n",
    "    else:\n",
    "        normal_map_opt = texture.create_trainable(ref_mesh.material['normal'], FLAGS.texture_res, not FLAGS.custom_mip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad56766-2401-4296-bdf8-2b23fb671d0b",
   "metadata": {},
   "source": [
    "#### 创建可训练 Kd,Ks 反照率和高光纹理\n",
    "Setup Kd, Ks albedo and specular textures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fe73f24-98ff-4055-b969-e19bf0338fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if FLAGS.random_textures:\n",
    "    if FLAGS.layers > 1:\n",
    "        kd_map_opt = texture.create_trainable(np.random.uniform(size=FLAGS.texture_res + [4], low=0.0, high=1.0), FLAGS.texture_res, not FLAGS.custom_mip)\n",
    "    else:\n",
    "        kd_map_opt = texture.create_trainable(np.random.uniform(size=FLAGS.texture_res + [3], low=0.0, high=1.0), FLAGS.texture_res, not FLAGS.custom_mip)\n",
    "\n",
    "    ksR = np.random.uniform(size=FLAGS.texture_res + [1], low=0.0, high=0.01)\n",
    "    ksG = np.random.uniform(size=FLAGS.texture_res + [1], low=FLAGS.min_roughness, high=1.0)\n",
    "    ksB = np.random.uniform(size=FLAGS.texture_res + [1], low=0.0, high=1.0)\n",
    "    ks_map_opt = texture.create_trainable(np.concatenate((ksR, ksG, ksB), axis=2), FLAGS.texture_res, not FLAGS.custom_mip)\n",
    "else:\n",
    "    kd_map_opt = texture.create_trainable(ref_mesh.material['kd'], FLAGS.texture_res, not FLAGS.custom_mip)\n",
    "    ks_map_opt = texture.create_trainable(ref_mesh.material['ks'], FLAGS.texture_res, not FLAGS.custom_mip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fc40ee1-8c61-4ad5-a51d-abfe7fcc1c32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "反照率贴图： torch.Size([1, 2048, 2048, 3])\n",
      "高光贴图：   torch.Size([1, 2048, 2048, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"反照率贴图：\", kd_map_opt.data.shape)\n",
    "# showImageTensor(kd_map_opt.data)\n",
    "print(\"高光贴图：  \", ks_map_opt.data.shape)\n",
    "# showImageTensor(ks_map_opt.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2996b5-2bdb-43a0-94e0-89025929c508",
   "metadata": {},
   "source": [
    "#### 创建位移贴图\n",
    "Trainable displacement map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0672a5de-7d14-46e0-b7c4-9e985770b5d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "displacement_map_var = None\n",
    "if FLAGS.subdivision > 0:\n",
    "    displacement_map_var = torch.tensor(np.zeros(FLAGS.texture_res + [1], dtype=np.float32), dtype=torch.float32, device='cuda', requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e79de70-5baa-4cd5-b395-a367a7857be1",
   "metadata": {},
   "source": [
    "#### 根据配置信息组合训练参数\n",
    "Add trainable arguments according to config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf6021e4-4ba2-40ff-9069-12fe1bda4e88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not 'position' in FLAGS.skip_train:\n",
    "    trainable_list += [v_pos_opt]        \n",
    "if not 'normal' in FLAGS.skip_train:\n",
    "    trainable_list += normal_map_opt.getMips()\n",
    "if not 'kd' in FLAGS.skip_train:\n",
    "    trainable_list += kd_map_opt.getMips()\n",
    "if not 'ks' in FLAGS.skip_train:\n",
    "    trainable_list += ks_map_opt.getMips()\n",
    "if not 'displacement' in FLAGS.skip_train and displacement_map_var is not None:\n",
    "    trainable_list += [displacement_map_var]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdf763d-e24f-44ca-87bc-39ae31804aac",
   "metadata": {},
   "source": [
    "## 配置用于网格优化的材质\n",
    "Setup material for optimized mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7091b5d7-4c0f-48b4-af4f-be9d76c8e58a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "opt_material = {\n",
    "    'bsdf'   : ref_mesh.material['bsdf'],\n",
    "    'kd'     : kd_map_opt,\n",
    "    'ks'     : ks_map_opt,\n",
    "    'normal' : normal_map_opt\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29cbaeb-0680-400f-9df7-069ba585f4b6",
   "metadata": {},
   "source": [
    "## 配置参考网格\n",
    "Setup reference mesh. Compute tangentspace and animate with skinning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7b219e6-947e-45f5-b253-92ddb503a415",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3609, 0.2383],\n",
       "        [0.3599, 0.2383],\n",
       "        [0.3604, 0.2388],\n",
       "        ...,\n",
       "        [0.1388, 0.8414],\n",
       "        [0.1393, 0.8409],\n",
       "        [0.1388, 0.8404]], device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "render_ref_mesh.input.eval({}).v_tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "159e330b-c698-46ca-9765-f6734a842cf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imesh.t_tex_idx tensor([[      0,       1,       2],\n",
      "        [      3,       4,       5],\n",
      "        [      6,       7,       8],\n",
      "        ...,\n",
      "        [2205093, 2205094, 2205095],\n",
      "        [2205096, 2205097, 2205098],\n",
      "        [2205099, 2205100, 2205101]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "render_ref_mesh = mesh.compute_tangents(ref_mesh)\n",
    "\n",
    "# Compute AABB of reference mesh. Used for centering during rendering TODO: Use pre frame AABB?\n",
    "ref_mesh_aabb = mesh.aabb(render_ref_mesh.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6e5782-0704-42dc-82b1-d1e73bf42a11",
   "metadata": {},
   "source": [
    "## 配置基础网格及预计算\n",
    "Setup base mesh operation graph, precomputes topology etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f601e93-9272-4843-8717-338f74ce6650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create optimized mesh with trainable positions \n",
    "opt_base_mesh = Mesh(v_pos_opt, normalized_base_mesh.t_pos_idx, material=opt_material, base=normalized_base_mesh)\n",
    "\n",
    "# Scale from [-1, 1] local coordinate space to match extents of the reference mesh\n",
    "opt_base_mesh = mesh.align_with_reference(opt_base_mesh, ref_mesh)\n",
    "\n",
    "# Compute smooth vertex normals\n",
    "opt_base_mesh = mesh.auto_normals(opt_base_mesh)\n",
    "\n",
    "# Set up tangent space\n",
    "opt_base_mesh = mesh.compute_tangents(opt_base_mesh)\n",
    "\n",
    "# Subdivide if we're doing displacement mapping\n",
    "if FLAGS.subdivision > 0:\n",
    "    # Subdivide & displace optimized mesh\n",
    "    subdiv_opt_mesh = mesh.subdivide(opt_base_mesh, steps=FLAGS.subdivision)\n",
    "    opt_detail_mesh = mesh.displace(subdiv_opt_mesh, displacement_map_var, FLAGS.displacement, keep_connectivity=True)\n",
    "else:\n",
    "    opt_detail_mesh = opt_base_mesh\n",
    "\n",
    "# Laplace regularizer\n",
    "if FLAGS.relative_laplacian:\n",
    "    with torch.no_grad():\n",
    "        orig_opt_base_mesh = opt_base_mesh.eval().clone()\n",
    "    lap_loss_fn = regularizer.laplace_regularizer_const(opt_detail_mesh, orig_opt_base_mesh)\n",
    "else:\n",
    "    lap_loss_fn = regularizer.laplace_regularizer_const(opt_detail_mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36703212-1ae9-4885-a3e6-cd0640fc0707",
   "metadata": {},
   "source": [
    "## 配置优化器\n",
    "Setup torch optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9aa12829-9f5c-461b-ab7c-7564ce3a6595",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer  = torch.optim.Adam(trainable_list, lr=FLAGS.learning_rate)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda x: max(0.0, 10**(-x*0.0002))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222639b3-211c-4600-a590-776e7726fc50",
   "metadata": {},
   "source": [
    "## 创建图像损失\n",
    "Image loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58ad7614-7fcb-44ba-b363-bcd6eb9263da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_loss_fn = createLoss(FLAGS)\n",
    "\n",
    "# Background color\n",
    "if FLAGS.background == 'checker':\n",
    "    background = torch.tensor(util.checkerboard(FLAGS.display_res, 8), dtype=torch.float32, device='cuda')\n",
    "elif FLAGS.background == 'white':\n",
    "    background = torch.ones((1, FLAGS.display_res, FLAGS.display_res, 3), dtype=torch.float32, device='cuda')\n",
    "else:\n",
    "    background = None\n",
    "\n",
    "showImageTensor(background)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ab845b-32ce-437a-8477-7e893c2266ad",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59a36604-622b-4006-8061-aada17d978c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getRenderParams(flags):\n",
    "    eye = np.array(flags.camera_eye)\n",
    "    up  = np.array(flags.camera_up)\n",
    "    at  = np.array([0,0,0])\n",
    "    a_mv =  util.lookAt(eye, at, up)                                      # 模型视图（Model View）\n",
    "    a_mvp = np.matmul(flags.proj_mtx, a_mv).astype(np.float32)[None, ...] # 模型视图投影（Model View Projection）\n",
    "    a_lightpos = np.linalg.inv(a_mv)[None, :3, 3]\n",
    "    a_campos = np.linalg.inv(a_mv)[None, :3, 3]\n",
    "    return {'mvp' : a_mvp, 'lightpos' : a_lightpos, 'campos' : a_campos, 'resolution' : [FLAGS.display_res, FLAGS.display_res], 'time' : 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dada0d72-021b-469f-b0f1-210d3576d585",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def renderScene(glctx, opt_base_mesh, opt_detail_mesh, render_ref_mesh, ref_mesh_aabb, background, displacement_map_var, FLAGS, mesh_scale, params):\n",
    "    # Center meshes\n",
    "    _opt_detail = mesh.center_by_reference(opt_detail_mesh.eval(params), ref_mesh_aabb, mesh_scale)\n",
    "    _opt_ref    = mesh.center_by_reference(render_ref_mesh.eval(params), ref_mesh_aabb, mesh_scale)\n",
    "\n",
    "    # Render\n",
    "    if FLAGS.subdivision > 0:\n",
    "        _opt_base   = mesh.center_by_reference(opt_base_mesh.eval(params), ref_mesh_aabb, mesh_scale)\n",
    "        img_base = render.render_mesh(glctx, _opt_base, params['mvp'], params['campos'], params['lightpos'], FLAGS.light_power, FLAGS.display_res, \n",
    "            num_layers=FLAGS.layers, background=background, min_roughness=FLAGS.min_roughness)\n",
    "        img_base = util.scale_img_nhwc(img_base, [FLAGS.display_res, FLAGS.display_res])\n",
    "\n",
    "    img_opt = render.render_mesh(glctx, _opt_detail, params['mvp'], params['campos'],  params['lightpos'], FLAGS.light_power, FLAGS.display_res, \n",
    "        num_layers=FLAGS.layers, background=background, min_roughness=FLAGS.min_roughness)\n",
    "    img_ref = render.render_mesh(glctx, _opt_ref, params['mvp'], params['campos'],  params['lightpos'], FLAGS.light_power, FLAGS.display_res, \n",
    "        num_layers=1, spp=FLAGS.spp, background=background, min_roughness=FLAGS.min_roughness)\n",
    "\n",
    "    # Rescale\n",
    "    img_opt  = util.scale_img_nhwc(img_opt,  [FLAGS.display_res, FLAGS.display_res])\n",
    "    img_ref  = util.scale_img_nhwc(img_ref,  [FLAGS.display_res, FLAGS.display_res])\n",
    "\n",
    "    if FLAGS.subdivision > 0:\n",
    "        img_disp = torch.clamp(torch.abs(displacement_map_var[None, ...]), min=0.0, max=1.0).repeat(1,1,1,3)\n",
    "        img_disp = util.scale_img_nhwc(img_disp, [FLAGS.display_res, FLAGS.display_res])\n",
    "        result_image = torch.cat([img_base, img_opt, img_ref], axis=2)\n",
    "    else:\n",
    "        result_image = torch.cat([img_opt, img_ref], axis=2)\n",
    "    return result_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fdffa3af-2ac8-490a-a788-e4643632ece7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def displayAndSaveImage(glctx, opt_base_mesh, opt_detail_mesh, render_ref_mesh, ref_mesh_aabb, background, displacement_map_var, mesh_scale, img_cnt, it, FLAGS):\n",
    "    display_image = FLAGS.display_interval and (it % FLAGS.display_interval == 0)\n",
    "    save_image    = FLAGS.save_interval    and (it % FLAGS.save_interval == 0)\n",
    "\n",
    "    params = getRenderParams(FLAGS)\n",
    "    with torch.no_grad():\n",
    "        result_image = renderScene(glctx, opt_base_mesh, opt_detail_mesh, render_ref_mesh, ref_mesh_aabb, background, displacement_map_var, FLAGS, mesh_scale, params)\n",
    "\n",
    "    result_image[0] = util.tonemap_srgb(result_image[0])\n",
    "    np_result_image = result_image[0].detach().cpu().numpy()\n",
    "    \n",
    "    if display_image:\n",
    "        util.display_image(np_result_image, size=FLAGS.display_res, title='%d / %d' % (it, FLAGS.iter))\n",
    "    if save_image:\n",
    "        util.save_image(FLAGS.out_dir + '/' + ('img_%06d.png' % img_cnt), np_result_image)\n",
    "        img_cnt = img_cnt+1\n",
    "    return img_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed2b16-778d-4ef4-aa52-1ce6f873bc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================================\n",
    "#  训练循环\n",
    "#  Training Loop\n",
    "# ==============================================================================================\n",
    "img_cnt = 0        # 图片计数\n",
    "ang = 0.0          #\n",
    "img_loss_vec = []  # 图片损失向量集\n",
    "lap_loss_vec = []  #\n",
    "iter_dur_vec = []  #\n",
    "mesh_scale=2.0\n",
    "\n",
    "log_interval = 10\n",
    "\n",
    "glctx = dr.RasterizeGLContext() # 可微渲染管线\n",
    "for it in range(FLAGS.iter+1):\n",
    "    \n",
    "    # ==============================================================================================\n",
    "    #  展示/保存输出。在最初执行用以获得初始网格数据。\n",
    "    #  Display / save outputs. Do it before training so we get initial meshes\n",
    "    # ==============================================================================================\n",
    "    img_cnt = displayAndSaveImage(glctx, opt_base_mesh, opt_detail_mesh, render_ref_mesh, ref_mesh_aabb, \n",
    "                                  background, displacement_map_var, mesh_scale, img_cnt, it, FLAGS)    \n",
    "    # ==============================================================================================\n",
    "    #  训练初始化\n",
    "    #  Initailize Training\n",
    "    # ==============================================================================================\n",
    "    iter_start_time = time.time()                                    # 记录迭代开始时间\n",
    "    img_loss = torch.zeros([1], dtype=torch.float32, device='cuda')  # 初始化图像空间损失\n",
    "    lap_loss = torch.zeros([1], dtype=torch.float32, device='cuda')  # 初始化laplace损失\n",
    "\n",
    "    iter_res = FLAGS.train_res # 初始化分辨率\n",
    "    iter_spp = FLAGS.spp       # 初始化单像素采样率\n",
    "    if FLAGS.random_train_res:\n",
    "        # 分辨率随机化\n",
    "        # Random resolution, 16x16 -> train_res. Scale up sample count so we always land close to train_res*samples_per_pixel samples\n",
    "        iter_res = np.random.randint(16, FLAGS.train_res+1)\n",
    "        iter_spp = FLAGS.spp * (FLAGS.train_res // iter_res)\n",
    " \n",
    "    mvp      = np.zeros((FLAGS.batch, 4,4), dtype=np.float32) # 模型视图投影矩阵（Model View Projection）\n",
    "    campos   = np.zeros((FLAGS.batch, 3),   dtype=np.float32) # 相机位置（Camera Position）\n",
    "    lightpos = np.zeros((FLAGS.batch, 3),   dtype=np.float32) # 光源位置（Light Position）\n",
    "\n",
    "    # ==============================================================================================\n",
    "    #  构建用于minibatching的变换栈\n",
    "    #  Build transform stack for minibatching\n",
    "    # ==============================================================================================\n",
    "    for b in range(FLAGS.batch):\n",
    "        # 随机旋转/平移矩阵用于优化\n",
    "        # Random rotation/translation matrix for optimization.\n",
    "        r_rot      = util.random_rotation_translation(0.25)                 # 随机渲染矩阵\n",
    "        r_mv       = np.matmul(util.translate(0, 0, -FLAGS.radius), r_rot)  # 随机模型视图\n",
    "        mvp[b]     = np.matmul(FLAGS.proj_mtx, r_mv).astype(np.float32)     # 随机模型视图投影\n",
    "        campos[b]  = np.linalg.inv(r_mv)[:3, 3]                             # 随机相机位置\n",
    "        lightpos[b] = util.cosine_sample(campos[b])*FLAGS.radius            # 随机光源位置\n",
    "    params = {'mvp' : mvp, 'lightpos' : lightpos, 'campos' : campos, 'resolution' : [iter_res, iter_res], 'time' : 0}\n",
    "    # 随机背景颜色\n",
    "    # Random bg color\n",
    "    randomBgColor = torch.rand(FLAGS.batch, iter_res, iter_res, 3, dtype=torch.float32, device='cuda')\n",
    "    # ==============================================================================================\n",
    "    #  居中对齐所有网格\n",
    "    #  Evaluate all mesh ops (may change when positions are modified etc) and center/align meshes\n",
    "    # ==============================================================================================\n",
    "    _opt_ref  = mesh.center_by_reference(render_ref_mesh.eval(params), ref_mesh_aabb, mesh_scale)\n",
    "    _opt_detail = mesh.center_by_reference(opt_detail_mesh.eval(params), ref_mesh_aabb, mesh_scale)\n",
    "    # ==============================================================================================\n",
    "    #  渲染参考网格\n",
    "    #  Render reference mesh\n",
    "    # ==============================================================================================\n",
    "    with torch.no_grad():\n",
    "        color_ref = render.render_mesh(glctx, _opt_ref, mvp, campos, lightpos, FLAGS.light_power, iter_res, spp=iter_spp, num_layers=1, \n",
    "                                       background=randomBgColor, min_roughness=FLAGS.min_roughness)\n",
    "    # ==============================================================================================\n",
    "    #  渲染训练网格\n",
    "    #  Render the trainable mesh\n",
    "    # ==============================================================================================\n",
    "    color_opt = render.render_mesh(glctx, _opt_detail, mvp, campos, lightpos, FLAGS.light_power, iter_res, spp=iter_spp, num_layers=FLAGS.layers, \n",
    "                                   msaa=True , background=randomBgColor, min_roughness=FLAGS.min_roughness)\n",
    "    # ==============================================================================================\n",
    "    #  损失计算\n",
    "    #  Compute loss\n",
    "    # ==============================================================================================\n",
    "    # 计算图像空间损失\n",
    "    img_loss = image_loss_fn(color_opt, color_ref)\n",
    "    # 计算laplace损失\n",
    "    lap_loss = lap_loss_fn.eval(params)\n",
    "\n",
    "    # Debug, store every training iteration\n",
    "    # result_image = torch.cat([color_opt, color_ref], axis=2)\n",
    "    # np_result_image = result_image[0].detach().cpu().numpy()\n",
    "    # util.save_image(out_dir + '/' + ('train_%06d.png' % it), np_result_image)\n",
    "\n",
    "    # 记录损失\n",
    "    img_loss_vec.append(img_loss.item())\n",
    "    lap_loss_vec.append(lap_loss.item())\n",
    "\n",
    "    # Schedule for laplacian loss weight\n",
    "    if it == 0:\n",
    "        if FLAGS.laplacian_factor is not None:\n",
    "            lap_fac = FLAGS.laplacian_factor\n",
    "        else:\n",
    "            ratio = 0.1 / lap_loss.item() # Hack that assumes RMSE ~= 0.1\n",
    "            lap_fac = ratio * 0.25\n",
    "        min_lap_fac = lap_fac * 0.02\n",
    "    else:\n",
    "        lap_fac = (lap_fac - min_lap_fac) * 10**(-it*0.000001) + min_lap_fac\n",
    "\n",
    "    # Compute total aggregate loss\n",
    "    total_loss = img_loss + lap_loss * lap_fac\n",
    "\n",
    "    # ==============================================================================================\n",
    "    #  反向传播（Backpropagate）\n",
    "    # ==============================================================================================\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    # ==============================================================================================\n",
    "    #  将训练参数裁剪至合理区间（Clamp trainables to reasonable range）\n",
    "    # ==============================================================================================\n",
    "    normal_map_opt.clamp_(min=-1, max=1)\n",
    "    kd_map_opt.clamp_(min=0, max=1)\n",
    "    ks_map_opt.clamp_rgb_(minR=0, maxR=1, minG=FLAGS.min_roughness, maxG=1.0, minB=0.0, maxB=1.0)\n",
    "    iter_dur_vec.append(time.time() - iter_start_time)\n",
    "    # ==============================================================================================\n",
    "    #  保存日志（Log & save outputs）\n",
    "    # ==============================================================================================\n",
    "    if log_interval and (it % log_interval == 0):\n",
    "        with torch.no_grad():\n",
    "            showBatchImageTensor(color_ref, FLAGS.batch)\n",
    "            showBatchImageTensor(color_opt, FLAGS.batch)\n",
    "        \n",
    "        img_loss_avg = np.mean(np.asarray(img_loss_vec[-log_interval:]))\n",
    "        lap_loss_avg = np.mean(np.asarray(lap_loss_vec[-log_interval:]))\n",
    "        iter_dur_avg = np.mean(np.asarray(iter_dur_vec[-log_interval:]))\n",
    "        remaining_time = (FLAGS.iter-it)*iter_dur_avg\n",
    "        print(\"iter=%5d, img_loss=%.6f, lap_loss=%.6f, lr=%.5f, time=%.1f ms, rem=%s\" % \n",
    "            (it, img_loss_avg, lap_loss_avg*lap_fac, optimizer.param_groups[0]['lr'], iter_dur_avg*1000, util.time_to_text(remaining_time)))\n",
    "\n",
    "# Save final mesh to file\n",
    "obj.write_obj(os.path.join(FLAGS.out_dir, \"mesh/\"), opt_base_mesh.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f65bbf-0f1d-40a9-bbc4-387a570c0d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
